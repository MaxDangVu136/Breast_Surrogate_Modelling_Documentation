{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Contributors: Dr. Gonzalo Maso Talou, Stephen Creamer, Max Dang Vu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document is designed to inform people how to use the \"run_learn_model.py\" machine learning function. \n",
    "By the end of reading this document the user should be able to train their own machine learning model through the use of this algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This machine learning workflow consists of three components:\n",
    "\n",
    "- A training model (\"run_learn_model.py\")\n",
    "- A config file (\"data_generation_DS_HPC.gen\")\n",
    "- A dataset (../resources/train_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics of machine learning:\n",
    "Machine learning is an optimisation technique where a ML model is trying to learn the correlations to match between the input and outputs in a sample, to find what is representative of the overall system. \n",
    "This is done through the use of a loss function which weighs how close the ML acting on the inputs gets to the outputs. Depending on how the loss function is made it can be weighed for different objectives e.g. to ensure one of the parameters is as close to zero as possible or more commonly an even weighting between each parameter/condition.\n",
    "To minimise the loss function a descent method is used. Many common descent methods are gradient based methods where the gradient at the current point is calculated and decides how far and in which direction the next measure should be taken. As this would go on indefinitely a decay is put onto the gradient descent to slowly reach a minima.\n",
    "For effective testing the dataset should be split into a training set and a testing and/or verification set which does not contain overlapping data - the training dataset is used to train the model while the testing dataset is used to take a measure of the model's quality.\n",
    "\n",
    "This is just a short overview of common things in machine learning and many other loss functions, descent methods and models exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specific machine learning:\n",
    "The machine learning model used by \"run_learn_model.py\" is a surrogate model. A surrogate model is one which utilises another mdoel to calculate best output and then learns off the back of it. \n",
    "The loss function used is a siamese loss function which means it has a weighting of multiple different factors. In the default setting this combines three adjustable weights to the domain, neumann boundary, and dirichlet boundary allowing for extra weighting on the boundary. \n",
    "The model is a densely connected three layer neural network with a 64,64,64,1 structure. Densely connected networks mean that each node in each layer is connected to every node in the layer before and after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The config file (fig 1.):\n",
    "The configuration file controls the inputs into the function and model as well as the parameters of the machine learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example Config File](config_file.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs:\n",
    "The default(example) model has eight inputs (x, y, z, E1, E2, pc1w, pc2w, pressure) and three outputs (x,y,z). The model takes in a large dataset of related inputs and outputs and trains itself off of the knowledge of the correct output based on each combination of inputs. The trained model takes in the point location (x,y,z), the elasticity (E1, E2), two PCA weights (pc1w, pc2w) and a pressure (P) and returns the displacement at the point in three dimensions (x', y', z')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters:\n",
    "The model also has parameters that affect how the model runs and operates. These are:\n",
    "\n",
    "- epochs: Is how many times the full dataset is run through the neural network\n",
    "- batch_size: Is the size of each batch of data - as the amount of data is often too large to run at once it is split into randomised batches \n",
    "- learning_rate: Decides how far to travel in each step with a low learning rate being prone to long solve times and getting stuck in local minima and a large learning rate leading to instability. \n",
    "- weights: Decide the weighting on each of the domain, neumann boundary and dirichlet boundary\n",
    "- decay_per_epoch: Decides how the learning rate alters between each epoch with the learning rate being calculated by the current learning rate * the decay\n",
    "- condition_BN: Neumann boundary condition\n",
    "- values_BN: Neumann boundary condition\n",
    "- condition_BD: Dirichlet boundary condition\n",
    "- values_BD: Dirichlet boundary condition\n",
    "- samples per file: States how many samples are in each file\n",
    "- testing_ratio: The ratio of testing to training data \n",
    "- labels: Labels the inputs and outputs \n",
    "- inputs: Number of inputs to the model\n",
    "- outputs: Number of outputs to the model\n",
    "- neurons_ann: The arrangement of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset (fig 2.):\n",
    "The dataset should be in the format matching the labels with a full .csv file. To add more inputs just add more columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dataset Image](dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using BioMeS\n",
    "\n",
    "To use BioMeS you need the aforementioned three files, these can be obtained by either asking Gonzalo or from:\n",
    "\n",
    "Dataset for multiple material parameters: /hpc/gmas685/workspaces/openCMISS-workspace/heart-biomechanics-id/examples/machine_learning_mechanics/training/results/train_0/\n",
    "Configuration files: /hpc_atog/gmas685/workspaces/python-workspace/BioMeS/resources/\n",
    "All files: Either github or\n",
    "/hpc_atog/gmas685/workspaces/python-workspace/BioMeS - May be out of date\n",
    "\n",
    "Then under BioMeS make a new folder titled \"resources\" and put the data_generation_DS_HPC.gen in the folder. Open the file and change the training path (The bottom one) to a path of your choice :D \n",
    "\n",
    "\n",
    "To monitor activity in the gpus:\n",
    "- watch -n 1 nvidia-smi  - Updates every second and gives current status\n",
    "- gpustat - Shows which are currently in use etc. key stats\n",
    "\n",
    "Chose a GPU which is not being used\n",
    "\n",
    "Then the following set of commands should be ran in the terminal (for hpc7):\n",
    " \n",
    "- module load cuda/cuda-10.1 -which loads the parallel computing for running on the GPUs\n",
    "- source /hpc/gmas685/programs/tensorflow_2_1_HPC7/bin/activate -which executes and starts tensorflow which should change your terminal\n",
    "- cd /#your_folder_for_BioMeS#/BioMeS/scripts/ -move to your folder which contains the run_learn_model.py script\n",
    "- CUDA_VISIBLE_DEVICES=6 PYTHONPATH=/#your_folder_for_BioMeS#/ python run_learn_model.py ../resources/data_generation_DS_HPC.gen - Set which GPU to use (the one you decided on earlier (example=6)), set the environment variable (PYTHONPATH=), run the model (run_learn_model.py), and put the location and name of the configuration file (data_generation_DS_HPC.gen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-92ce6aa09763>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-92ce6aa09763>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    CUDA_VISIBLE_DEVICES=7 PYTHONPATH=/hpc/scre583/ML/BioMeS python run_learn_model.py /hpc/scre583/opt/papers/paper_1/gen_files/training_50000_6.gen\u001b[0m\n\u001b[1;37m                           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "CUDA_VISIBLE_DEVICES=7 PYTHONPATH=/hpc/scre583/ML/BioMeS python run_learn_model.py /hpc/scre583/opt/papers/paper_1/gen_files/training_50000_6.gen"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
